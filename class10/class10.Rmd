---
title: "Class 10: Unsupervised Learning Mini-Project"
author: "Larissa Rapadas"
date: "2/6/2020"
output: github_document
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Analysis of Human Breast Cancer Cells

First we need to import (i.e. read) our input data.

```{r}
# Read the data and store as wisc.df
wisc.df <- read.csv("WisconsinCancer.csv")

# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[,3:32])

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id

# Create diagnosis vector for later 
diagnosis <- c(wisc.df$diagnosis)
```

### Questions:

Q1. How many observations are in this dataset?
```{r}
nrow(wisc.df)
```

Q2. How many of the observations have a malignant diagnosis?
```{r}
table(wisc.df[,"diagnosis"])
```
Q3. How many variables/features in the data are suffixed with _mean?
```{r}
mean <- grep("_mean",colnames(wisc.data))
length (mean)
```

## Principal Component Analysis

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to **determine if the data should be scaled**. 

```{r}
# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data,2,sd)
```
Looks like we need to scale. So we must set `scale=TRUE` in our `prcomp()` call
```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale=TRUE )
summary(wisc.pr)
```
### Questions:

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

**- 44.27%**

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

**- 3 PCs**

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

**- 7 PCs**

## Interpreting PCA results

Create a biplot of the wisc.pr using the biplot() function.
```{r}
biplot(wisc.pr)
```
### Questions:

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

**It's very difficult to understand because all of the ID names and column names are used to plot the points on this graph. Considering the size of our data, it is not very helpful for our speific data set.**

This sucks. So we make our own PC1 vs PC2 plot and let's color the points by diagnosis (malignant and benign).

```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1], wisc.pr$x[,2],
     col=wisc.df$diagnosis,
     xlab = "PC1", ylab = "PC2")
```
### Questions:

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

## Cluster in PC space

First, let's see if we can cluster the original data
```{r}
wisc.hc <- hclust(dist(wisc.data))
plot(wisc.hc)
```
Let's input the PCA results instead to get a better structured cluster plot. Let's try and combine the results of PCA with clustering
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:3]),
                         method="ward.D2")
plot(wisc.pr.hclust)
```
```{r}
grps3 <- cutree(wisc.pr.hclust, k=3)
table(grps3)
```
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps3)
```
---
title: 'Class10: Unsupervised Learning Mini-Project'
author: "Barry Grant"
date: "2/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Analysis of Human Breast Cancer Cells

First we need to import (i.e. read) our input data. I downloaded the CSV file from the class website: https://bioboot.github.io/bimm143_S18/class-material/WisconsinCancer.csv

```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
head(wisc.df)
```

There are some funky things in this dataset that we will ignore for our analysis. This includes the first and second ID and Diagnosis columns and the funny last X column (col 33).

```{r}
# Convert the features of the data: wisc.data
wisc.data <- as.matrix( wisc.df[,3:32] )
head(wisc.data)
```

> Q. How many patients do we have data for?

```{r}
nrow(wisc.data)
```

> Q. How many cancer and non-cancer?

```{r}
table(wisc.df$diagnosis)
```

> Q. How many variables/features in the data are suffixed with _mean?

For this I will turn to the `grep()` function and look at the help page to see how it works.

```{r}

grep("_mean", colnames(wisc.data), value=TRUE )
```

```{r}
length( grep("_mean", colnames(wisc.data) ) )
```

## Principal Component Analysis

Before we turn to PCA we need to think, or consider, whether we should SCALE our input.

It is important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data include:

The input variables use different units of measurement.

- The input variables have significantly different variances.
- Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the `colMeans()` and `apply()` functions like you’ve done before.

```{r}
round( apply(wisc.data, 2, sd ), 2)
```


Looks like we need to set scale=TRUE!!

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale=TRUE )
summary(wisc.pr)
```


> Q. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

The 1st PC captures 44.27% of the original variance. Note that 72.6% is captured in the first 3 PCs..


Lets make some figures...

```{r}
biplot(wisc.pr)
```

That is a hot mess! We need to do our own PC1 vs PC2 plot and lets color by the diagnosis.

```{r}
attributes(wisc.pr)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=wisc.df$diagnosis)
abline(h=0, col="gray", lty=2)
abline(v=0, col="gray", lty=2)
```

### Cluster in PC space

First lets see if we can cluster the origional data

```{r}
wisc.hc <- hclust( dist(wisc.data) )
plot(wisc.hc)
```

This does not look good! Let's try and combine the results of PCA with clustering...

Let’s see if PCA improves or degrades the performance of hierarchical clustering.

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

```{r}
wisc.pr.hclust <- hclust( dist(wisc.pr$x[,1:3]), method="ward.D2" )
plot(wisc.pr.hclust)
```

To get our clusters out of this tree we need to CUT it with the `cutree()` function.

```{r}
grps3 <- cutree(wisc.pr.hclust, k=2)
table(grps3)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps3)
```

We can use the `table()` function to compare the $diagnosis vector with our cluster results vector.

```{r}
table(grps3, wisc.df$diagnosis)
```

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
new
```
```{r}
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2],
     col=wisc.df$diagnosis)

# Now let's add these new points we just read into our plot
points(npc[,1], npc[,2], col="blue", pch=15, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```


